import time
from selenium import webdriver
from selenium.common import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec
import pandas as pd
import csv
import re
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import threading
import sys
import os


class ScraperGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Ø³ÛŒØ³ØªÙ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ø³ØªÙˆØ±Ø§Ù†")
        self.root.geometry("500x400")
        self.root.configure(bg='#f5f5f5')

        self.setup_gui()
        self.driver = None
        self.current_csv_file = None
        self.current_dataframe = None

    def setup_gui(self):
        """ØªÙ†Ø¸ÛŒÙ… Ø±Ø§Ø¨Ø· Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ"""
        main_frame = ttk.Frame(self.root, padding=20)
        main_frame.pack(fill='both', expand=True)

        # Ø¹Ù†ÙˆØ§Ù†
        title_label = ttk.Label(main_frame, text="ğŸ½ï¸ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ø³ØªÙˆØ±Ø§Ù†",
                                font=('Tahoma', 16, 'bold'))
        title_label.pack(pady=(0, 30))

        # ÙÛŒÙ„Ø¯ Ù…Ø­Ù„Ù‡
        neighborhood_frame = ttk.Frame(main_frame)
        neighborhood_frame.pack(fill='x', pady=10)

        ttk.Label(neighborhood_frame, text="Ù†Ø§Ù… Ù…Ø­Ù„Ù‡:", font=('Tahoma', 12)).pack(side='left')
        self.neighborhood_var = tk.StringVar()
        neighborhood_entry = ttk.Entry(neighborhood_frame, textvariable=self.neighborhood_var,
                                       font=('Tahoma', 12), width=30)
        neighborhood_entry.pack(side='left', padx=(10, 0), fill='x', expand=True)

        # ÙÛŒÙ„Ø¯ Ù†ÙˆØ¹ ØºØ°Ø§
        food_frame = ttk.Frame(main_frame)
        food_frame.pack(fill='x', pady=10)

        ttk.Label(food_frame, text="Ù†ÙˆØ¹ ØºØ°Ø§:", font=('Tahoma', 12)).pack(side='left')
        self.food_var = tk.StringVar()
        food_entry = ttk.Entry(food_frame, textvariable=self.food_var,
                               font=('Tahoma', 12), width=30)
        food_entry.pack(side='left', padx=(10, 0), fill='x', expand=True)

        # Ø¯Ú©Ù…Ù‡ Ø´Ø±ÙˆØ¹
        self.start_button = ttk.Button(main_frame, text="Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§",
                                       command=self.start_scraping)
        self.start_button.pack(pady=20)

        # Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')

        # ÙˆØ¶Ø¹ÛŒØª
        self.status_var = tk.StringVar(value="Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹...")
        status_label = ttk.Label(main_frame, textvariable=self.status_var,
                                 font=('Tahoma', 10))
        status_label.pack(pady=10)

    def start_scraping(self):
        """Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Ø¯Ø± ÛŒÚ© thread Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
        neighborhood = self.neighborhood_var.get().strip()
        food = self.food_var.get().strip()

        if not neighborhood or not food:
            messagebox.showerror("Ø®Ø·Ø§", "Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ù…Ø­Ù„Ù‡ Ùˆ Ù†ÙˆØ¹ ØºØ°Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯")
            return

        # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¯Ú©Ù…Ù‡ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
        self.start_button.config(state='disabled')
        self.progress.pack(pady=10)
        self.progress.start()
        self.status_var.set("Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±...")

        # Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ Ø¯Ø± thread Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        thread = threading.Thread(target=self.run_scraping, args=(neighborhood, food))
        thread.daemon = True
        thread.start()

    def run_scraping(self, neighborhood, food):
        """Ø§Ø¬Ø±Ø§ÛŒ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯"""
        try:
            self.status_var.set("Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±...")
            driver = setup_driver(neighborhood_name=neighborhood, food_name=food)

            if driver:
                self.status_var.set("Ø¯Ø± Ø­Ø§Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
                scraped_data = scraper(driver)

                if scraped_data:
                    self.status_var.set("Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
                    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                    cleaned_data = clean_and_validate_data(scraped_data)

                    # Ø§ÛŒØ¬Ø§Ø¯ DataFrame
                    df = pd.DataFrame(cleaned_data)
                    self.current_dataframe = df  # Ø°Ø®ÛŒØ±Ù‡ dataframe Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø¹Ø¯ÛŒ

                    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ CSV
                    filename = f"{neighborhood}_{food}_structured.csv"
                    df.to_csv(filename, index=False, encoding='utf-8-sig')

                    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ØªØ­Ù„ÛŒÙ„
                    self.current_csv_file = filename

                    self.status_var.set(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filename}")

                    # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                    self.show_summary_page(df)

                else:
                    self.status_var.set("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†Ø´Ø¯")
                    messagebox.showinfo("Ø§Ø·Ù„Ø§Ø¹", "Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø±Ø³ØªÙˆØ±Ø§Ù†â€ŒÙ‡Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†Ø´Ø¯.")

            else:
                self.status_var.set("Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±")
                messagebox.showerror("Ø®Ø·Ø§", "Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±")

        except Exception as e:
            self.status_var.set(f"Ø®Ø·Ø§: {str(e)}")
            messagebox.showerror("Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}")
        finally:
            # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª Ø¨Ù‡ Ø­Ø§Ù„Øª Ø¹Ø§Ø¯ÛŒ
            self.progress.stop()
            self.progress.pack_forget()
            self.start_button.config(state='normal')

    def show_summary_page(self, df):
        """Ù†Ù…Ø§ÛŒØ´ ØµÙØ­Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        try:
            summary_window = tk.Toplevel(self.root)
            summary_window.title("Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡")
            summary_window.geometry("600x500")
            summary_window.configure(bg='#f5f5f5')
            summary_window.transient(self.root)
            summary_window.grab_set()

            # Ø¹Ù†ÙˆØ§Ù†
            title_label = ttk.Label(summary_window, text="ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡",
                                    font=('Tahoma', 16, 'bold'))
            title_label.pack(pady=20)

            # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯Ø± ÛŒÚ© ÙØ±ÛŒÙ…
            info_frame = ttk.Frame(summary_window)
            info_frame.pack(fill='both', expand=True, padx=20, pady=10)

            # Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
            total_comments = len(df)
            total_restaurants = df['restaurant_name'].nunique()
            comments_with_rating = df[df['rating'] != ''].shape[0]

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
            rating_data = df[df['rating'] != '']['rating']
            if not rating_data.empty:
                try:
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯
                    numeric_ratings = pd.to_numeric(rating_data, errors='coerce')
                    avg_rating = numeric_ratings.mean()
                except:
                    avg_rating = 0
            else:
                avg_rating = 0

            # Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªÙ† Ø®Ù„Ø§ØµÙ‡
            summary_text = f"""
ğŸ“ˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:

â€¢ ğŸ¢ ØªØ¹Ø¯Ø§Ø¯ Ø±Ø³ØªÙˆØ±Ø§Ù†â€ŒÙ‡Ø§: {total_restaurants} 
â€¢ ğŸ’¬ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ø¸Ø±Ø§Øª: {total_comments}
â€¢ â­ Ù†Ø¸Ø±Ø§Øª Ø¯Ø§Ø±Ø§ÛŒ Ø§Ù…ØªÛŒØ§Ø²: {comments_with_rating}
â€¢ ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§: {avg_rating:.2f}
â€¢ ğŸ’¾ ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {self.current_csv_file}

ğŸ“‹ Ù„ÛŒØ³Øª Ø±Ø³ØªÙˆØ±Ø§Ù†â€ŒÙ‡Ø§:
"""

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø±Ø³ØªÙˆØ±Ø§Ù†â€ŒÙ‡Ø§
            restaurants = df['restaurant_name'].unique()
            for i, restaurant in enumerate(restaurants[:10], 1):  # ÙÙ‚Ø· 10 Ø±Ø³ØªÙˆØ±Ø§Ù† Ø§ÙˆÙ„
                restaurant_comments = df[df['restaurant_name'] == restaurant].shape[0]
                summary_text += f"  {i}. {restaurant} ({restaurant_comments} Ù†Ø¸Ø±)\n"

            if len(restaurants) > 10:
                summary_text += f"  ... Ùˆ {len(restaurants) - 10} Ø±Ø³ØªÙˆØ±Ø§Ù† Ø¯ÛŒÚ¯Ø±"

            # Ù†Ù…Ø§ÛŒØ´ Ù…ØªÙ† Ø®Ù„Ø§ØµÙ‡
            text_widget = scrolledtext.ScrolledText(info_frame,
                                                    font=('Tahoma', 11),
                                                    wrap=tk.WORD,
                                                    height=15)
            text_widget.pack(fill='both', expand=True)
            text_widget.insert('1.0', summary_text)
            text_widget.config(state='disabled')

            # ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
            button_frame = ttk.Frame(summary_window)
            button_frame.pack(pady=20)

            # Ø¯Ú©Ù…Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
            analyze_button = ttk.Button(button_frame,
                                        text="ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§",
                                        command=lambda: self.open_detailed_analysis(summary_window),
                                        style='Accent.TButton')
            analyze_button.pack(side='left', padx=10)

            # Ø¯Ú©Ù…Ù‡ Ø¨Ø³ØªÙ†
            close_button = ttk.Button(button_frame,
                                      text="Ø¨Ø³ØªÙ†",
                                      command=summary_window.destroy)
            close_button.pack(side='left', padx=10)

            # Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¯Ú©Ù…Ù‡ Ø§ØµÙ„ÛŒ
            style = ttk.Style()
            style.configure('Accent.TButton', font=('Tahoma', 11, 'bold'))

        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ù…Ø§ÛŒØ´ ØµÙØ­Ù‡ Ø®Ù„Ø§ØµÙ‡: {e}")

    def open_detailed_analysis(self, summary_window):
        """Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±"""
        if not self.current_csv_file:
            messagebox.showerror("Ø®Ø·Ø§", "Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
            return

        try:
            # Ø¨Ø³ØªÙ† Ù¾Ù†Ø¬Ø±Ù‡ Ø®Ù„Ø§ØµÙ‡
            summary_window.destroy()

            self.status_var.set("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±...")

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù…Ø§Ú˜ÙˆÙ„ Ø¯ÙˆÙ… Ø¨Ù‡ sys.path
            current_dir = os.path.dirname(os.path.abspath(__file__))
            if current_dir not in sys.path:
                sys.path.append(current_dir)

            # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„
            from nlp2 import run_analysis_from_scraper

            # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© thread Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
            analysis_thread = threading.Thread(target=self.execute_analysis, args=(self.current_csv_file,))
            analysis_thread.daemon = True
            analysis_thread.start()

        except ImportError as e:
            self.status_var.set(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„: {str(e)}")
            messagebox.showerror("Ø®Ø·Ø§", f"Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯: {str(e)}")
        except Exception as e:
            self.status_var.set(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„: {str(e)}")
            messagebox.showerror("Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}")

    def execute_analysis(self, csv_file):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
        try:
            from nlp2 import run_analysis_from_scraper
            # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ØªØ­Ù„ÛŒÙ„ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ ÙØ§ÛŒÙ„
            run_analysis_from_scraper(csv_file)

        except Exception as e:
            self.root.after(0, lambda: self.status_var.set(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„: {str(e)}"))
            self.root.after(0, lambda: messagebox.showerror("Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}"))


# region Driver Setup
def setup_driver(neighborhood_name, food_name):
    url = "https://www.snappfood.ir/"
    driver = webdriver.Edge()
    driver.get(url)
    driver.maximize_window()
    time.sleep(5)

    try:
        neighborhood_search_box_xpath = '''//*[@id="__next"]/div/div/main/div[1]/div[2]/div[2]/div[3]/div/p'''
        neighborhood_search_box = driver.find_element(By.XPATH, neighborhood_search_box_xpath)
        neighborhood_search_box.click()
        time.sleep(10)

        neighborhood_finder_xpath = '''//*[@id="modal-backdrop"]/div/section/div/section/form/div[2]/div/input'''
        neighborhood_finder = driver.find_element(By.XPATH, neighborhood_finder_xpath)
        neighborhood_finder.click()
        neighborhood_finder.clear()
        neighborhood_finder.send_keys(neighborhood_name + ' ')
        time.sleep(10)

        neighborhood_search_result_xpath = '''//*[@id="modal-backdrop"]/div/section/div/section/div/button[1]/p[2]'''
        neighborhood_search_result = driver.find_element(By.XPATH, neighborhood_search_result_xpath)
        neighborhood_search_result.click()
        time.sleep(5)
        # Ø¯Ú©Ù…Ù‡ ØªØ§ÛŒÛŒØ¯ Ø§Ø¯Ø±Ø³
        neighborhood_confirmation_button_xpath = '''//*[@id="modal-backdrop"]/div/form/div/button'''
        neighborhood_confirmation_button = driver.find_element(By.XPATH, neighborhood_confirmation_button_xpath)
        neighborhood_confirmation_button.click()
        time.sleep(10)

        food_search_box_xpath = '''//*[@id="__next"]/div/div/div[1]/header/div[1]/div[2]/p'''
        food_search_box = driver.find_element(By.XPATH, food_search_box_xpath)
        food_search_box.click()
        time.sleep(2)
        # Ù‚Ø³Ù…Øª Ø³Ø±Ú†
        food_input_xpath = '''//*[@id="modal-backdrop"]/div/div/div[1]/input'''
        food_input = driver.find_element(By.XPATH, food_input_xpath)
        food_input.click()
        food_input.send_keys(food_name + ' ')
        time.sleep(5)

        view_all_xpath = '''//*[@id="modal-backdrop"]/div/div/div[2]/div[2]/div/a/div/span'''
        view_all = driver.find_element(By.XPATH, view_all_xpath)
        view_all.click()
        time.sleep(10)

        return driver

    except NoSuchElementException:
        print("The page requested is not found.")
        return None


# endregion Driver Setup

def extract_comments_grouped(comments_elements):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø¸Ø±Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ (Ù‡Ø± Ù†Ø¸Ø± Ø¯Ø± 3 Ø®Ø·)
    """
    grouped_comments = []

    for i in range(0, len(comments_elements), 3):
        if i + 2 < len(comments_elements):
            # Ø³Ù‡ Ø®Ø· Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© Ù†Ø¸Ø±
            date_line = comments_elements[i].text.strip()
            rating_line = comments_elements[i + 1].text.strip()
            comment_line = comments_elements[i + 2].text.strip()

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÛŒØªÛŒÙ†Ú¯ Ø§Ø² Ø®Ø· Ø¯ÙˆÙ…
            rating = None
            if rating_line.isdigit() and 1 <= int(rating_line) <= 5:
                rating = int(rating_line)
            elif 'Ø³ØªØ§Ø±Ù‡' in rating_line or 'Ø§Ø²' in rating_line:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÛŒØªÛŒÙ†Ú¯ Ø§Ø² Ù…ØªÙ†
                numbers = re.findall(r'\d+', rating_line)
                if numbers and 1 <= int(numbers[0]) <= 5:
                    rating = int(numbers[0])

            grouped_comments.append({
                'date': date_line,
                'rating': rating,
                'comment': comment_line
            })

    return grouped_comments


# region Scraper Function
def scraper(driver):
    # --- Your scrolling logic ---
    last_height = driver.execute_script('return document.body.scrollHeight')
    while True:
        driver.execute_script('window.scrollBy(0,800)')
        time.sleep(2)
        new_height = driver.execute_script('return document.body.scrollHeight')
        if new_height == last_height:
            print("Reached bottom of page.")
            break
        last_height = new_height

    # --- Setup for the loop ---
    div_container_xpath = '''//*[@id="__next"]/div/main/div[1]'''
    item_css_selector = ".sc-citwmv.jOCtGV"  # Corrected: space replaced with dot
    wait = WebDriverWait(driver, 10)

    wait.until(ec.visibility_of_element_located((By.XPATH, div_container_xpath)))
    num_items = len(driver.find_elements(By.CSS_SELECTOR, item_css_selector))

    if num_items == 0:
        print("No items found.")
        return []  # Return an empty list

    print(f"Found {num_items} items. Starting loop...")

    all_comments_data = []
    original_window = driver.current_window_handle

    # --- Use the "Index Loop" ---
    for i in range(num_items):
        print(f"--- Processing item {i + 1} of {num_items} ---")
        try:
            all_items = driver.find_elements(By.CSS_SELECTOR, item_css_selector)

            if i >= len(all_items):
                print("   > Item list changed. Stopping.")
                break
            item_to_click = all_items[i]

            print("   > Opening in new tab...")
            ActionChains(driver) \
                .key_down(Keys.CONTROL) \
                .click(item_to_click) \
                .key_up(Keys.CONTROL) \
                .perform()

            wait.until(ec.number_of_windows_to_be(2))
            new_tab = [window for window in driver.window_handles if window != original_window][0]
            driver.switch_to.window(new_tab)

            print(f"   > Switched to new tab: {driver.title}")

            try:
                ITEM_NAME_SELECTOR = (By.TAG_NAME, "h1")
                comment_container_xpath = '''//*[@id="modal-backdrop"]/div/div[2]/div[3]'''
                comment_selector_css = ".sc-hKgILt.hmsjTi"

                item_name_element = wait.until(ec.visibility_of_element_located(ITEM_NAME_SELECTOR))
                item_name = item_name_element.text

                comment_container = wait.until(ec.visibility_of_element_located((By.XPATH, comment_container_xpath)))

                comment_elements = comment_container.find_elements(By.CSS_SELECTOR, comment_selector_css)
                print(f"   > Found {len(comment_elements)} comment lines for '{item_name}'.")

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø¸Ø±Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡
                grouped_comments = extract_comments_grouped(comment_elements)
                print(f"   > Extracted {len(grouped_comments)} complete comments.")

                for comment_data in grouped_comments:
                    all_comments_data.append({
                        "restaurant_name": item_name,
                        "comment_text": comment_data['comment'],
                        "date": comment_data['date'],
                        "rating": comment_data['rating'] if comment_data['rating'] else ""
                    })

            except Exception as e:
                print(f"   > Error scraping details from new tab: {e}")

            driver.close()
            driver.switch_to.window(original_window)
            print("   > Closed tab and returned to main page.")
        except Exception as e:
            print(f"   > Error on item {i + 1}: {e}")
            if len(driver.window_handles) > 1:
                driver.close()
            driver.switch_to.window(original_window)
        time.sleep(1)
    print("Loop finished.")
    return all_comments_data


# endregion Scraper Function

def clean_and_validate_data(data):
    """
    Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    """
    cleaned_data = []

    for item in data:
        # ÙÙ‚Ø· Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ù†Ø§Ù… Ø±Ø³ØªÙˆØ±Ø§Ù† Ùˆ Ù…ØªÙ† Ù†Ø¸Ø± Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
        if item.get('restaurant_name') and item.get('comment_text'):
            cleaned_data.append({
                "restaurant_name": item["restaurant_name"].strip(),
                "comment_text": item["comment_text"].strip(),
                "date": item.get("date", "").strip(),
                "rating": item.get("rating", "")
            })

    return cleaned_data


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø±Ø§Ø¨Ø· Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ"""
    root = tk.Tk()
    app = ScraperGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()